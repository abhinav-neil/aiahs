{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detection\n",
    "This notebook performs hate speech detection on a dataset of tweets. The pipeline is as follows:\n",
    "- Load & preprocess data\n",
    "- Finetune a pretrained RoBERTa model for hate speech classification\n",
    "- Evaluate the model on the test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/aiahs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# standard\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'    # for debugging\n",
    "load_dotenv()\n",
    "import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# custom\n",
    "from src.utils import *\n",
    "from src.models import HateSpeechClassifier\n",
    "from src.train import train_pl\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(f'using device: {device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_memory_usage(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    total_buffers = sum(p.numel() for p in model.buffers())\n",
    "    total_memory = (total_params + total_buffers) * 4  # assuming float32 parameters\n",
    "    return total_memory / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "def get_batch_memory_usage(dataloader):\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        # Calculate memory for both tensors\n",
    "        batch_memory = (input_ids.numel() + attention_mask.numel()) * 4  # assuming float32 data\n",
    "        return batch_memory / (1024 ** 2)  # Convert bytes to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del train_loader, val_loader, test_loader\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainer args:\n",
      "state: TrainerState(status=<TrainerStatus.INITIALIZING: 'initializing'>, fn=None, stage=None)\n",
      "barebones: False\n",
      "_data_connector: <pytorch_lightning.trainer.connectors.data_connector._DataConnector object at 0x7fb9b0708b90>\n",
      "_accelerator_connector: <pytorch_lightning.trainer.connectors.accelerator_connector._AcceleratorConnector object at 0x7fb9b296ef50>\n",
      "_logger_connector: <pytorch_lightning.trainer.connectors.logger_connector.logger_connector._LoggerConnector object at 0x7fb9b09e0210>\n",
      "_callback_connector: <pytorch_lightning.trainer.connectors.callback_connector._CallbackConnector object at 0x7fb9aff45890>\n",
      "_checkpoint_connector: <pytorch_lightning.trainer.connectors.checkpoint_connector._CheckpointConnector object at 0x7fb9b04ab5d0>\n",
      "_signal_connector: <pytorch_lightning.trainer.connectors.signal_connector._SignalConnector object at 0x7fb9b04abed0>\n",
      "fit_loop: <pytorch_lightning.loops.fit_loop._FitLoop object at 0x7fb9b04ab750>\n",
      "validate_loop: <pytorch_lightning.loops.evaluation_loop._EvaluationLoop object at 0x7fb9aff45710>\n",
      "test_loop: <pytorch_lightning.loops.evaluation_loop._EvaluationLoop object at 0x7fb9b04ab410>\n",
      "predict_loop: <pytorch_lightning.loops.prediction_loop._PredictionLoop object at 0x7fb9b04a8290>\n",
      "accumulate_grad_batches: 1\n",
      "_default_root_dir: /home/neil/aiahs\n",
      "callbacks: [<pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0x7fb9b3149c10>, <pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x7fb9b04ab0d0>, <pytorch_lightning.callbacks.model_summary.ModelSummary object at 0x7fb9b04ab150>, <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fb9b1c72650>]\n",
      "datamodule: None\n",
      "check_val_every_n_epoch: 1\n",
      "reload_dataloaders_every_n_epochs: 0\n",
      "gradient_clip_val: None\n",
      "gradient_clip_algorithm: None\n",
      "_detect_anomaly: False\n",
      "should_stop: False\n",
      "profiler: <pytorch_lightning.profilers.base.PassThroughProfiler object at 0x7fb9b0497e50>\n",
      "_loggers: [<pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x7fb9b05a3f90>]\n",
      "log_every_n_steps: 5\n",
      "fast_dev_run: False\n",
      "overfit_batches: 0.0\n",
      "limit_train_batches: 1.0\n",
      "limit_val_batches: 1.0\n",
      "limit_test_batches: 1.0\n",
      "limit_predict_batches: 1.0\n",
      "num_sanity_val_steps: 2\n",
      "val_check_interval: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/opt/conda/envs/aiahs/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/neil/aiahs/model_ckpts exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | RobertaModel       | 355 M \n",
      "1 | fc       | Linear             | 2.0 K \n",
      "2 | loss     | CrossEntropyLoss   | 0     \n",
      "3 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "355 M     Trainable params\n",
      "0         Non-trainable params\n",
      "355 M     Total params\n",
      "1,421.447 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 310/310 [02:46<00:00,  1.87it/s, v_num=2]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  66%|██████▋   | 206/310 [01:46<00:53,  1.94it/s, v_num=2]"
     ]
    }
   ],
   "source": [
    "# clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# create data loaders\n",
    "data_path = 'data/hs_davidson2017.csv'\n",
    "bsz = 64\n",
    "max_len = 256\n",
    "num_workers = 12\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "train_loader, val_loader, test_loader = create_data_loaders(data_path, tokenizer, max_len, bsz, num_workers)\n",
    "\n",
    "# create the model\n",
    "model = HateSpeechClassifier('roberta-large', num_labels=2)\n",
    "# model = model.to(device)\n",
    "\n",
    "# check gpu memory usage\n",
    "# model_memory = get_model_memory_usage(model)\n",
    "# print(f\"Model Memory Usage: {model_memory:.2f} MB\")\n",
    "# train_batch_memory = get_batch_memory_usage(train_loader)\n",
    "# val_batch_memory = get_batch_memory_usage(val_loader)\n",
    "# print(f\"Train Batch Memory Usage: {train_batch_memory:.2f} MB\")\n",
    "# print(f\"Validation Batch Memory Usage: {val_batch_memory:.2f} MB\")\n",
    "\n",
    "# training args\n",
    "args = {'num_epochs': 10, 'patience': 3, 'ckpt_name': 'ckpt_best_new', 'precision': '16-mixed'}\n",
    "\n",
    "# train\n",
    "trainer, _ = train_pl(model, train_loader, val_loader, args)\n",
    "\n",
    "# test\n",
    "trainer.test(test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.2208, test acc: 0.9423\n"
     ]
    }
   ],
   "source": [
    "# load the best model\n",
    "model = HateSpeechClassifier('roberta-large', num_labels=2)\n",
    "checkpoint = torch.load('checkpoints/ckpt_best.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_loss, test_accuracy = evaluate(model, test_data_loader, loss_fn, device)\n",
    "print(f'test loss: {test_loss:.4f}, test acc: {test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiahs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
